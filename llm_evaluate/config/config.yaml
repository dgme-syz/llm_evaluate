data:
  name: math_train      
  data_path: /home/nfs06/shenyz/data/SimpleRL/hard/train_processed.parquet
  subset_name: null
  split: train
  num_samples: null
  builder: parquet

evaluate:
  val_size: 4262
  num_proc: null
  metrics:
    - union_math_accuracy
  eval_func: base
  merge_strategy: pass                                               

use_server: false

# /home/nfs05/shenyz/verl/checkpoints/verl_grpo/Qwen2.5-1.5B+n_7+Qwen3-8B+256+gpus_8/global_step_160/actor/huggingface
# /home/nfs05/shenyz/verl/checkpoints/verl_grpo/Qwen2.5-1.5B+GRPO+256+gpus_4/global_step_460/actor/huggingface
# /home/nfs06/shenyz/models/Qwen2.5-1.5B
exp_name_prefix: "(SimpleRL_Greedy_7B)"
llm:
  model: /home/nfs05/shenyz/models/Qwen-2.5-7B-SimpleRL-Zoo/
  tokenizer: /home/nfs05/shenyz/models/Qwen-2.5-7B-SimpleRL-Zoo/
  trust_remote_code: true
  dtype: bfloat16 # 59
  gpu_memory_utilization: 0.9
  tensor_parallel_size: 2
  seed: 42
  enable_chunked_prefill: true # for V100, set false
  

server: {}

sample_params:
  offline:
    temperature: 0.0 #0.0
    max_tokens: 4096
    # top_p: 0.95 # 1.0
    # top_k: 20 # -1
    # repetition_penalty: 1.1 # 1
    n: 1

  online: {}


save_outputs: true # save format: jsonl
outputs_dir: ./outputs/1/rollout


hydra:
  run:
    dir: ./outputs/1/rollout

# simple rl greedy decode 0.57
# simple rl greedy 0.6 t 0.57
