data:
  # - data/translation/mixed_train_data
  # - data/translation/wmt25_en2zh
  - data/translation/wmt24_en2zh
  - data/translation/challenge_set_en2zh
  - data/translation/flores_en2zh
  # - data/translation/flores_data_sample
  # - data/translation/wmt24_data_sample
  # - data/translation/challenge_set_data_sample
  # - data/translation/challenge_set_zh2en
  # - data/translation/wmt24_en2id
  # - data/translation/flores_en2ja
  # - data/translation/flores_zh2ja
  # - data/translation/flores_zh2hr
  # - data/translation/data_sample

evaluate:
  val_size: 4096
  num_proc: null
  metrics:
    - sentenceBLEU
    - BLEU
    - cometkiwi
    - BLEURT
  metrics_args:
    ignore_columns: [0]
  eval_func: recheck
  eval_func_args:
    check_num: 1
    use_thinking: true
    use_thinking_prompts: false
  merge_strategy: total

use_server: false

llm:
  prompt_template: qwen_chat_mt
  exp_tag: default
  vllm:
    model: /home/nfs05/shenyz/models/Qwen3-0.6B
    tokenizer: /home/nfs05/shenyz/models/Qwen3-0.6B
    trust_remote_code: true
    dtype: bfloat16
    gpu_memory_utilization: 0.8
    tensor_parallel_size: 4
    seed: 42
    enable_chunked_prefill: false 
    enable_prefix_caching: false
    max_model_len: 16384

tokenize_args:
  enable_thinking: false

server: {}

generate_params:
  offline:
    use_beam_search: false
    sampling_params:
      max_tokens: 8196
      # top_k: 20
      # top_p: 0.95
      temperature: 0.6
      # repetition_penalty: 1.0
      seed: 42
      n: 1
    beam_search_params:
      beam_width: 4
      max_tokens: 1024
  online: 
    max_tokens: 512
    


save_outputs: true # save format: jsonl
outputs_dir: ./outputs/trained/

